from typing import Dict, List, Tuple, Set, Any, Generator
import json
from collections import defaultdict, Counter
import math
import os
from tqdm import tqdm

class LayoutAnalyzer:
    def __init__(self, layout_config: Dict[str, Any]):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Ä–∞—Å–∫–ª–∞–¥–∫–∏
        """
        self.layout_data = layout_config.get("layout", {})
        self.hand_map = {}
        self.position_map = {}
        self.finger_map = {}
        self.column_map = {}  # –ë—É–∫–≤–∞ -> —Å—Ç–æ–ª–±–µ—Ü
        
        self._parse_layout()
    
    def _parse_layout(self):
        """–ü–∞—Ä—Å–∏—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ä–∞—Å–∫–ª–∞–¥–∫–∏ –∏ —Å–æ–∑–¥–∞–µ—Ç –º–∞–ø–ø–∏–Ω–≥–∏"""
        for letter, data in self.layout_data.items():
            if len(data) >= 3:
                hand, row, col = data[0], data[1], data[2]
                self.hand_map[letter] = hand
                self.position_map[letter] = (hand, row, col)
                self.column_map[letter] = col
                finger = self._get_finger_for_column(hand, col)
                self.finger_map[letter] = finger
    
    def _get_finger_for_column(self, hand: str, column: int) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–∞–ª–µ—Ü –¥–ª—è —Å—Ç–æ–ª–±—Ü–∞"""
        if column <= 2:
            return f"{hand}y"  # —É–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–π
        elif column <= 4:
            return f"{hand}s"  # —Å—Ä–µ–¥–Ω–∏–π
        elif column <= 6:
            return f"{hand}b"  # –±–µ–∑—ã–º—è–Ω–Ω—ã–π
        else:
            return f"{hand}m"  # –º–∏–∑–∏–Ω–µ—Ü
    
    def analyze_sequence_comfort_new_logic(self, sequence: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–¥–æ–±—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –ù–û–í–û–ô –ª–æ–≥–∏–∫–µ:
        - –†–∞–∑–Ω—ã–µ —Ä—É–∫–∏ ‚Üí –ù–ï–£–î–û–ë–ù–û ‚ùå
        - –õ–µ–≤–∞—è —Ä—É–∫–∞: —É–¥–æ–±–Ω–æ –∫–æ–≥–¥–∞ —Å—Ç–æ–ª–±—Ü—ã –í–û–ó–†–ê–°–¢–ê–Æ–¢ (–º–∏–∑–∏–Ω–µ—Ü ‚Üí —É–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–π)
        - –ü—Ä–∞–≤–∞—è —Ä—É–∫–∞: —É–¥–æ–±–Ω–æ –∫–æ–≥–¥–∞ —Å—Ç–æ–ª–±—Ü—ã –£–ë–´–í–ê–Æ–¢ (–º–∏–∑–∏–Ω–µ—Ü ‚Üí —É–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã–π)
        - –û–¥–∏–Ω –ø–∞–ª–µ—Ü ‚Üí –ù–ï–£–î–û–ë–ù–û ‚ùå
        """
        if len(sequence) < 2:
            return {'comfort': 'unknown', 'reason': 'too_short'}
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –µ—Å—Ç—å –≤ —Ä–∞—Å–∫–ª–∞–¥–∫–µ
        if not all(char in self.position_map for char in sequence):
            return {'comfort': 'unknown', 'reason': 'unknown_chars'}
        
        hands = [self.hand_map[char] for char in sequence]
        unique_hands = set(hands)
        
        # –†–∞–∑–Ω—ã–µ —Ä—É–∫–∏ - –ù–ï–£–î–û–ë–ù–û ‚ùå
        if len(unique_hands) > 1:
            return {
                'comfort': 'uncomfortable',
                'reason': 'different_hands',
                'hand_type': 'both',
                'sequence': sequence,
                'length': len(sequence)
            }
        
        # –û–¥–Ω–∞ —Ä—É–∫–∞
        hand_type = 'left' if list(unique_hands)[0] == 'l' else 'right'
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –Ω–∞ –æ–¥–Ω–æ–º –ø–∞–ª—å—Ü–µ
        fingers = [self.finger_map[char] for char in sequence]
        if len(set(fingers)) == 1:
            return {
                'comfort': 'uncomfortable',
                'reason': 'same_finger',
                'hand_type': hand_type,
                'sequence': sequence,
                'length': len(sequence)
            }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑ 2+ —Å–∏–º–≤–æ–ª–æ–≤
        if len(sequence) >= 2:
            comfort_level = self._analyze_direction_comfort(sequence, hand_type)
            return {
                'comfort': comfort_level,
                'reason': 'direction_analysis',
                'hand_type': hand_type,
                'sequence': sequence,
                'length': len(sequence)
            }
        
        return {'comfort': 'unknown', 'reason': 'unable_to_analyze'}
    
    def _analyze_direction_comfort(self, sequence: str, hand_type: str) -> str:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–¥–æ–±—Å—Ç–≤–æ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        """
        comfortable_moves = 0
        total_moves = len(sequence) - 1
        
        for i in range(total_moves):
            char1, char2 = sequence[i], sequence[i+1]
            col1, col2 = self.column_map[char1], self.column_map[char2]
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–¥–æ–±–Ω–æ–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
            if hand_type == 'left':
                # –õ–µ–≤–∞—è —Ä—É–∫–∞: —É–¥–æ–±–Ω–æ –∫–æ–≥–¥–∞ —Å—Ç–æ–ª–±—Ü—ã –í–û–ó–†–ê–°–¢–ê–Æ–¢
                is_comfortable = col2 > col1
            else:
                # –ü—Ä–∞–≤–∞—è —Ä—É–∫–∞: —É–¥–æ–±–Ω–æ –∫–æ–≥–¥–∞ —Å—Ç–æ–ª–±—Ü—ã –£–ë–´–í–ê–Æ–¢  
                is_comfortable = col2 < col1
            
            if is_comfortable:
                comfortable_moves += 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å —É–¥–æ–±—Å—Ç–≤–∞
        comfort_ratio = comfortable_moves / total_moves if total_moves > 0 else 0
        
        if comfort_ratio >= 0.8:  # 80%+ —É–¥–æ–±–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            return 'comfortable'
        elif comfort_ratio >= 0.5:  # 50-79% —É–¥–æ–±–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            return 'partial'
        else:  # –ú–µ–Ω–µ–µ 50% —É–¥–æ–±–Ω—ã—Ö –¥–≤–∏–∂–µ–Ω–∏–π
            return 'uncomfortable'
    
    def analyze_word_sequences_comprehensive(self, word: str) -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –≤ —Å–ª–æ–≤–µ
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        """
        result = {
            'word': word,
            'word_length': len(word),
            'sequences_by_length': {
                2: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0, 'details': []},
                3: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0, 'details': []},
                4: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0, 'details': []},
                5: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0, 'details': []}
            },
            'sequences_by_hand': {
                'left': {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                'right': {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                'both': {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0}
            },
            'sequence_frequencies': Counter(),
            'covers_whole_word': False
        }
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –¥–ª–∏–Ω
        for seq_len in [2, 3, 4, 5]:
            if len(word) >= seq_len:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ–∫—Ä—ã–≤–∞–µ—Ç –ª–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤—Å—ë —Å–ª–æ–≤–æ
                if len(word) == seq_len:
                    result['covers_whole_word'] = True
                
                for i in range(len(word) - seq_len + 1):
                    sequence = word[i:i + seq_len]
                    analysis = self.analyze_sequence_comfort_new_logic(sequence)
                    
                    if analysis['comfort'] != 'unknown':
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–ª–∏–Ω–µ
                        result['sequences_by_length'][seq_len]['total'] += 1
                        result['sequences_by_length'][seq_len][analysis['comfort']] += 1
                        result['sequences_by_length'][seq_len]['details'].append(analysis)
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä—É–∫–∞–º
                        hand_type = analysis.get('hand_type', 'both')
                        result['sequences_by_hand'][hand_type]['total'] += 1
                        result['sequences_by_hand'][hand_type][analysis['comfort']] += 1
                        
                        # –°–æ–±–∏—Ä–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
                        result['sequence_frequencies'][sequence] += 1
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Counter –≤ dict
        result['sequence_frequencies'] = dict(result['sequence_frequencies'])
        
        return result
    
    def calculate_comprehensive_analysis(self, wordlist: List[str]) -> Dict[str, Any]:
        """
        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—Å–µ–≥–æ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤
        """
        total_stats = {
            'by_length': {
                2: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                3: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                4: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                5: {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0}
            },
            'by_hand': {
                'left': {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                'right': {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0},
                'both': {'total': 0, 'comfortable': 0, 'partial': 0, 'uncomfortable': 0}
            },
            'word_coverage': {
                'words_with_full_coverage': 0,  # –°–ª–æ–≤–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏
                'total_words': len(wordlist)
            },
            'sequence_frequencies': Counter(),
            'comfort_examples': {
                'comfortable': [],
                'partial': [],
                'uncomfortable': []
            },
            'word_length_stats': defaultdict(int)
        }
        
        for word in tqdm(wordlist, desc="–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑"):
            analysis = self.analyze_word_sequences_comprehensive(word)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏–Ω–µ —Å–ª–æ–≤–∞
            total_stats['word_length_stats'][len(word)] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–∫—Ä—ã—Ç–∏—é —Å–ª–æ–≤–∞
            if analysis['covers_whole_word']:
                total_stats['word_coverage']['words_with_full_coverage'] += 1
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–ª–∏–Ω–∞–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            for seq_len in [2, 3, 4, 5]:
                length_data = analysis['sequences_by_length'][seq_len]
                for comfort_type in ['comfortable', 'partial', 'uncomfortable']:
                    total_stats['by_length'][seq_len][comfort_type] += length_data[comfort_type]
                    total_stats['by_length'][seq_len]['total'] += length_data[comfort_type]
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ä—É–∫–∞–º
            for hand_type in ['left', 'right', 'both']:
                hand_data = analysis['sequences_by_hand'][hand_type]
                for comfort_type in ['comfortable', 'partial', 'uncomfortable']:
                    total_stats['by_hand'][hand_type][comfort_type] += hand_data[comfort_type]
                    total_stats['by_hand'][hand_type]['total'] += hand_data[comfort_type]
            
            # –°–æ–±–∏—Ä–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            for seq, freq in analysis['sequence_frequencies'].items():
                total_stats['sequence_frequencies'][seq] += freq
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —É–¥–æ–±—Å—Ç–≤–∞
            for seq_len in [2, 3, 4, 5]:
                for seq_analysis in analysis['sequences_by_length'][seq_len]['details']:
                    comfort_type = seq_analysis['comfort']
                    if len(total_stats['comfort_examples'][comfort_type]) < 10:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ 10 –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
                        total_stats['comfort_examples'][comfort_type].append(seq_analysis)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        for seq_len in [2, 3, 4, 5]:
            total = total_stats['by_length'][seq_len]['total']
            if total > 0:
                for comfort_type in ['comfortable', 'partial', 'uncomfortable']:
                    count = total_stats['by_length'][seq_len][comfort_type]
                    total_stats['by_length'][seq_len][f'{comfort_type}_percent'] = (count / total) * 100
        
        for hand_type in ['left', 'right', 'both']:
            total = total_stats['by_hand'][hand_type]['total']
            if total > 0:
                for comfort_type in ['comfortable', 'partial', 'uncomfortable']:
                    count = total_stats['by_hand'][hand_type][comfort_type]
                    total_stats['by_hand'][hand_type][f'{comfort_type}_percent'] = (count / total) * 100
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        total_stats['sequence_frequencies'] = dict(
            sorted(total_stats['sequence_frequencies'].items(), 
                  key=lambda x: x[1], reverse=True)
        )
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –æ–±—â—É—é —É–¥–æ–±–Ω–æ—Å—Ç—å
        total_comfortable = sum(total_stats['by_length'][l]['comfortable'] for l in [2, 3, 4, 5])
        total_sequences = sum(total_stats['by_length'][l]['total'] for l in [2, 3, 4, 5])
        
        total_stats['overall_comfort'] = {
            'comfortable': total_comfortable,
            'total_sequences': total_sequences,
            'comfort_percent': (total_comfortable / total_sequences * 100) if total_sequences > 0 else 0
        }
        
        return total_stats
    
    def prepare_plot_data(self, comprehensive_stats: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        """
        plot_data = {
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º –ø–æ –¥–ª–∏–Ω–∞–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
            'by_length': {
                'lengths': ['2 —Å–∏–º–≤–æ–ª–∞', '3 —Å–∏–º–≤–æ–ª–∞', '4 —Å–∏–º–≤–æ–ª–∞', '5 —Å–∏–º–≤–æ–ª–æ–≤'],
                'comfortable': [],
                'partial': [],
                'uncomfortable': [],
                'total_sequences': [],
                'comfortable_percent': [],
                'uncomfortable_percent': []
            },
            
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º –ø–æ —Ä—É–∫–∞–º
            'by_hand': {
                'hands': ['–õ–µ–≤–∞—è —Ä—É–∫–∞', '–ü—Ä–∞–≤–∞—è —Ä—É–∫–∞', '–û–±–µ —Ä—É–∫–∏'],
                'comfortable': [],
                'partial': [],
                'uncomfortable': [],
                'total_sequences': [],
                'comfortable_percent': [],
                'uncomfortable_percent': []
            },
            
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Å–ª–æ–≤
            'word_coverage': {
                'words_with_full_coverage': comprehensive_stats['word_coverage']['words_with_full_coverage'],
                'total_words': comprehensive_stats['word_coverage']['total_words'],
                'coverage_percent': (comprehensive_stats['word_coverage']['words_with_full_coverage'] / 
                                   comprehensive_stats['word_coverage']['total_words'] * 100) 
                                   if comprehensive_stats['word_coverage']['total_words'] > 0 else 0
            },
            
            # –¢–æ–ø –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            'top_sequences': {
                'most_frequent': list(comprehensive_stats['sequence_frequencies'].items())[:50],
                'most_comfortable': [],
                'most_uncomfortable': []
            },
            
            # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è –ª–µ–≥–µ–Ω–¥—ã –≥—Ä–∞—Ñ–∏–∫–æ–≤
            'examples': comprehensive_stats['comfort_examples'],
            
            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            'overall': {
                'comfort_percent': comprehensive_stats['overall_comfort']['comfort_percent'],
                'total_sequences': comprehensive_stats['overall_comfort']['total_sequences'],
                'total_words': comprehensive_stats['word_coverage']['total_words']
            }
        }
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –¥–ª–∏–Ω–∞–º
        for length in [2, 3, 4, 5]:
            data = comprehensive_stats['by_length'][length]
            plot_data['by_length']['comfortable'].append(data['comfortable'])
            plot_data['by_length']['partial'].append(data['partial'])
            plot_data['by_length']['uncomfortable'].append(data['uncomfortable'])
            plot_data['by_length']['total_sequences'].append(data['total'])
            plot_data['by_length']['comfortable_percent'].append(data.get('comfortable_percent', 0))
            plot_data['by_length']['uncomfortable_percent'].append(data.get('uncomfortable_percent', 0))
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ —Ä—É–∫–∞–º
        for hand_type, hand_name in [('left', '–õ–µ–≤–∞—è —Ä—É–∫–∞'), ('right', '–ü—Ä–∞–≤–∞—è —Ä—É–∫–∞'), ('both', '–û–±–µ —Ä—É–∫–∏')]:
            data = comprehensive_stats['by_hand'][hand_type]
            plot_data['by_hand']['comfortable'].append(data['comfortable'])
            plot_data['by_hand']['partial'].append(data['partial'])
            plot_data['by_hand']['uncomfortable'].append(data['uncomfortable'])
            plot_data['by_hand']['total_sequences'].append(data['total'])
            plot_data['by_hand']['comfortable_percent'].append(data.get('comfortable_percent', 0))
            plot_data['by_hand']['uncomfortable_percent'].append(data.get('uncomfortable_percent', 0))
        
        return plot_data


# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∞–π–ª–∞–º–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
def load_layout_from_json(file_path: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ä–∞—Å–∫–ª–∞–¥–∫–∏ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def read_words_by_lines(file_path: str, batch_size: int = 1000, 
                       encoding: str = 'utf-8') -> Generator[List[str], None, None]:
    """–ß–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª –ø–æ—Å—Ç—Ä–æ—á–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞—è –±–∞—Ç—á–∏ —Å–ª–æ–≤"""
    current_batch = []
    
    with open(file_path, 'r', encoding=encoding) as f:
        for line in f:
            line = line.strip()
            if line:
                words = line.split()
                current_batch.extend(words)
                
                if len(current_batch) >= batch_size:
                    yield current_batch
                    current_batch = []
    
    if current_batch:
        yield current_batch

def count_lines_in_file(file_path: str, encoding: str = 'utf-8') -> int:
    """–°—á–∏—Ç–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ñ–∞–π–ª–µ"""
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            return sum(1 for _ in f)
    except:
        return 0

def analyze_layout_comfort_from_file(layout_config: Dict[str, Any], 
                                   file_path: str, 
                                   file_type: str = 'words',
                                   batch_size: int = 1000,
                                   encoding: str = 'utf-8',
                                   max_samples: int = 10000) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —É–¥–æ–±–Ω–æ—Å—Ç—å —Ä–∞—Å–∫–ª–∞–¥–∫–∏ –∏–∑ —Ñ–∞–π–ª–∞ —Å –Ω–æ–≤—ã–º–∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏
    """
    analyzer = LayoutAnalyzer(layout_config)
    
    if file_type == 'words':
        total_words = count_lines_in_file(file_path, encoding)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        words_for_analysis = []
        word_generator = read_words_by_lines(file_path, batch_size, encoding)
        
        processed_words = 0
        pbar = tqdm(total=min(total_words, max_samples), desc="–°–±–æ—Ä —Å–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        try:
            for batch in word_generator:
                for word in batch:
                    if len(words_for_analysis) < max_samples:
                        words_for_analysis.append(word)
                        pbar.update(1)
                    processed_words += 1
                    
                    if len(words_for_analysis) >= max_samples:
                        break
                if len(words_for_analysis) >= max_samples:
                    break
        finally:
            pbar.close()
        
        print(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤: {len(words_for_analysis):,} –∏–∑ {processed_words:,}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        comprehensive_stats = analyzer.calculate_comprehensive_analysis(words_for_analysis)
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
        plot_data = analyzer.prepare_plot_data(comprehensive_stats)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'total_words_processed': processed_words,
            'words_analyzed': len(words_for_analysis),
            'overall_comfort_score': comprehensive_stats['overall_comfort']['comfort_percent'],
            'comprehensive_stats': comprehensive_stats,
            'plot_data': plot_data
        }
        
        return result
    
    else:
        raise ValueError("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ file_type='words'")

def print_analysis_summary(result):
    """
    –í—ã–≤–æ–¥–∏—Ç —Å–≤–æ–¥–∫—É –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    """
    plot_data = result['plot_data']
    
    print("=" * 80)
    print("–°–í–û–î–ö–ê –ê–ù–ê–õ–ò–ó–ê –î–õ–Ø –ì–†–ê–§–ò–ö–û–í")
    print("=" * 80)
    
    print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"  ‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–ª–æ–≤: {result['total_words_processed']:,}")
    print(f"  ‚Ä¢ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–ª–æ–≤: {result['words_analyzed']:,}")
    print(f"  ‚Ä¢ –û–±—â–∞—è —É–¥–æ–±–Ω–æ—Å—Ç—å: {result['overall_comfort_score']:.1f}%")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {plot_data['overall']['total_sequences']:,}")
    
    print(f"\nüìè –ì–ò–°–¢–û–ì–†–ê–ú–ú–´ –ü–û –î–õ–ò–ù–ê–ú –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô:")
    for i, length in enumerate(plot_data['by_length']['lengths']):
        print(f"  ‚Ä¢ {length}:")
        print(f"      –£–¥–æ–±–Ω—ã–µ: {plot_data['by_length']['comfortable'][i]:,} ({plot_data['by_length']['comfortable_percent'][i]:.1f}%)")
        print(f"      –ß–∞—Å—Ç–∏—á–Ω–æ —É–¥–æ–±–Ω—ã–µ: {plot_data['by_length']['partial'][i]:,}")
        print(f"      –ù–µ—É–¥–æ–±–Ω—ã–µ: {plot_data['by_length']['uncomfortable'][i]:,} ({plot_data['by_length']['uncomfortable_percent'][i]:.1f}%)")
        print(f"      –í—Å–µ–≥–æ: {plot_data['by_length']['total_sequences'][i]:,}")
    
    print(f"\nüëê –ì–ò–°–¢–û–ì–†–ê–ú–ú–´ –ü–û –†–£–ö–ê–ú:")
    for i, hand in enumerate(plot_data['by_hand']['hands']):
        print(f"  ‚Ä¢ {hand}:")
        print(f"      –£–¥–æ–±–Ω—ã–µ: {plot_data['by_hand']['comfortable'][i]:,} ({plot_data['by_hand']['comfortable_percent'][i]:.1f}%)")
        print(f"      –ß–∞—Å—Ç–∏—á–Ω–æ —É–¥–æ–±–Ω—ã–µ: {plot_data['by_hand']['partial'][i]:,}")
        print(f"      –ù–µ—É–¥–æ–±–Ω—ã–µ: {plot_data['by_hand']['uncomfortable'][i]:,} ({plot_data['by_hand']['uncomfortable_percent'][i]:.1f}%)")
        print(f"      –í—Å–µ–≥–æ: {plot_data['by_hand']['total_sequences'][i]:,}")
    
    print(f"\nüìù –ü–û–ö–†–´–¢–ò–ï –°–õ–û–í:")
    print(f"  ‚Ä¢ –°–ª–æ–≤–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏: {plot_data['word_coverage']['words_with_full_coverage']:,}")
    print(f"  ‚Ä¢ –í—Å–µ–≥–æ —Å–ª–æ–≤: {plot_data['word_coverage']['total_words']:,}")
    print(f"  ‚Ä¢ –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–∫—Ä—ã—Ç–∏—è: {plot_data['word_coverage']['coverage_percent']:.1f}%")
    
    print(f"\nüèÜ –¢–û–ü-5 –°–ê–ú–´–• –ß–ê–°–¢–´–• –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô:")
    for i, (seq, freq) in enumerate(plot_data['top_sequences']['most_frequent'][:5], 1):
        print(f"  {i}. '{seq}': {freq:,} —Ä–∞–∑")
    
    print(f"\n‚úÖ –ü–†–ò–ú–ï–†–´ –£–î–û–ë–ù–´–• –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô:")
    for example in plot_data['examples']['comfortable'][:3]:
        print(f"  ‚Ä¢ '{example['sequence']}' ({example['length']} —Å–∏–º–≤–æ–ª–∞, {example.get('hand_type', 'unknown')})")
    
    print(f"\n‚ùå –ü–†–ò–ú–ï–†–´ –ù–ï–£–î–û–ë–ù–´–• –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ï–ô:")
    for example in plot_data['examples']['uncomfortable'][:3]:
        print(f"  ‚Ä¢ '{example['sequence']}' ({example['length']} —Å–∏–º–≤–æ–ª–∞, {example.get('hand_type', 'unknown')}) - {example.get('reason', 'unknown')}")
    
    print("\n" + "=" * 80)
    print("–î–ê–ù–ù–´–ï –î–õ–Ø –ì–†–ê–§–ò–ö–û–í –ü–û–î–ì–û–¢–û–í–õ–ï–ù–´!")
    print("=" * 80)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª
def save_analysis_results(result, output_file: str = "layout_analysis_results_zubachev.json"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ JSON —Ñ–∞–π–ª"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}")

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å–∫–ª–∞–¥–∫–∏
    layout_config = load_layout_from_json("/Users/evgenii/Develop/py_proj/tr/KVA/example_layouts/zubachew_1.json")
    
    # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞
    print("–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞ —Å–æ —Å–ø–∏—Å–∫–æ–º —Å–ª–æ–≤:")
    result = analyze_layout_comfort_from_file(
        layout_config, 
        "/Users/evgenii/Develop/py_proj/tr/KVA/1grams-3.txt", 
        file_type='words',
        batch_size=1000,
        max_samples=1000000
    )
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print_analysis_summary(result)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    save_analysis_results(result)